# 目录

[TOC]



# 实验记录 2019-11-08

##可以读dblp的数据

之前尝试使用spark-xml读取dblp的数据，但是读取过程中发生OOM异常。

一次性读取全部dblp数据的代码如下：

```scala
    val df = spark.read
      .format("com.databricks.spark.xml")
      .option("rowTag", "dblp")
      .load("file:///root/dblp.xml")
```

这种读取的方式导致spark的Excutor端生成了过于庞大的SchemaRDD，导致了OOM异常。在查阅了spark-xml的README.md文件后，最终发现了正确的读取方式。



README.md中option支持的选项部分找到关于`rowTag`和`rootTag`的说明：

>* `rowTag`: The row tag of your xml files to treat as a row. For example, in this xml `<books> <book><book> ...</books>`, the appropriate value would be `book`. Default is `ROW`.
>* `rootTag`: The root tag of your xml files to treat as the root. For example, in this xml `<books> <book><book> ...</books>`, the appropriate value would be `books`. Default is `ROWS`.

简单的说`rowTag`选项是用来设置要将什么xml节点转换为DataFrame的row，而`rootTag`用来设置要将根节点设置为哪个节点。

这样就解释了为什么一次性读取dblp的代码会导致OOM异常。因为dblp.xml的根节点是dblp节点，但是却被当成了row。这会在Excutor端生成了一个巨大的RDD对象，导致OOM。



能够不导致OOM的参数设置如下：

```scala
    val df = spark.read
      .format("com.databricks.spark.xml")
      .option("rootTag", "dblp")
 			.option("rowTag", "article")
      .load("file:///root/dblp.xml")
```

避免一次读取全部dblp.xml数据，采用分别读取dblp根节点下的子节点的方式读取。



但是dblp数据集的大小有2g，我们没法知道dblp下一层的子节点都是什么？于是参考了dblp.dtd文件，得知了dblp下的字节点名称。并对各个字节点的记录数量进行了统计。

| subElement    | Count   |
| ------------- | ------- |
| article       | 2093906 |
| inproceedings | 2450274 |
| proceedings   | 41782   |
| book          | 17714   |
| incollection  | 59477   |
| phdthesis     | 73252   |
| mastersthesis | 12      |
| www           | 2346866 |
| person        | 0       |
| data          | 0       |

下一步准备将各个子节点写入单独的xml文件里

#实验记录 2019-11-14

##修改了dblp中的实体定义为latin-1编码

读取文件时发现出现了大量的记录显示corrupt_record

| _corrupt_record | _key | _mdate | _publtype | author | ee   |
| --------------- | ---- | ------ | --------- | ------ | ---- |
|                null|     tr/meltdown/s18|2018-01-07| informal|[Paul Kocher, Dan...|                null|https://spectreat...|
|<article mdate="2...|                null|      null|     null|                null|                null|                null|
|<article mdate="2...|                null|      null|     null|                null|                null|                null|


排查以后发现是由于dblp.xml存在一些xml实体定义导致spark-xml不能正常读取。

```xml
    <article mdate="2017-06-08" key="tr/ibm/IWBS191" publtype="informal">
        <author>Rolf Sander</author>
        <title>Die Repr&auml;sentation r&auml;umlichen Wissens und die Behandlung von Einbettungsproblemen mit Quadtreedepiktionen
        </title>
        <journal>IWBS Report</journal>
        <volume>191</volume>
        <year>1991</year>
        <publisher>IBM Germany Science Center, Institute for Knowledge Based Systems</publisher>
    </article>
```

查阅了spark-xml的readme没有发现可以根据dtd的实体定义替换为对应的latin-1编码字符的方法，于是打算将实体定义相关的字符串用正则表达式匹配并替换。读取dblp.xml替换相应的字符串以后写到dblp_after.xml文件中。

```scala
import java.io.File
import org.apache.spark.sql.SparkSession
object Test4 {
  val s1 = "file:////Users/gaoxin/WorkSpace/spark/dblp.xml"
  //将实体替换以后的dblp写在这里
  val s2 = "file:////Users/gaoxin/WorkSpace/spark/dblp_after.xml"

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("XML_Test")
      .master("local[*]")
      .getOrCreate()

    val subNode = Array("article")
    
    //文本方式读取
    val file = spark.sparkContext.textFile(s1)

    //转换实体
    val afterParse = file.map(s => ReplaceEntity.parse(s))
    
    //afterParse.foreach(it => println(it))

    val afterParedblp: File = new File(s2)

   if (afterParedblp.isFile) {
     println("delete "+ afterParedblp.delete())
   }

    afterParse.saveAsTextFile(s2)
  }
}
```



**将实体名称替换为实体编号的方式尽管可以在ide下正常显示，但是使用控制台的时候仍然显示乱码。应该替换为具体字符。下面的代码并没有根本地解决问题，以后需要改。**

```scala
import scala.util.matching.Regex

object ReplaceEntity {
  val regex: Regex = new Regex("&[A-Za-z]*;")
  def parse(s: String): String = {
    val optS = regex.replaceAllIn(s, it => it.toString() match {
      case "&reg;" => "&#174;"
      case "&micro;" => "&#181;"
      case "&times;" => "&#215;"
      case "&Agrave;" => "&#192;"
      case "&Aacute;" => "&#193;"
      case "&Acirc;" => "&#194;"
//.....省略
      case _ => {
        println(it)
        it.toString()
      }
    })

    optS
  }
}
```

经过实体替换以后的dblp_after.xml可以正常读取，检查了dataframe的记录也没有发现corrupt_record。

#实验记录 2019-11-21

可以进行简单的查询了

##选取特定的列

```scala
    import spark.implicits._
    dblpArticle.select($"title", $"author", $"url").show()
```

| title                | author               | url                  |
| -------------------- | -------------------- | -------------------- |
| Spectre Attacks: ... | [[Paul Kocher,,],... | null                 |
| Meltdown             | [[Moritz Lipp,,],... | null                 |
| An Evaluation of ... | [[Frank Manola,,]]   | db/labs/gte/index... |

## 根据long类型执行过滤

```scala
import spark.implicits._
dblpArticle.filter($"year" > 2000).show()
```

| title                | author               | year                 |
| -------------------- | -------------------- | -------------------- |
|Spectre Attacks: ...|[[Paul Kocher,,],...|2018|
|            Meltdown|[[Moritz Lipp,,],...|2018|

## 字符串正则匹配

```scala
import spark.implicits._
dblpArticle.select($"title", $"author", $"url").filter($"title" rlike "^Knowledge").show()
```
| title                | author               | year                 |
| -------------------- | -------------------- | -------------------- |
|Knowledge in Oper...|[[Toni Bollinger,...|null|

其他的写在测试代码里了

# 实验记录 2020-02-20

通过docker和docker-compose搭建了实验环境。

1. Mongodb环境
2. Spark和HDFS的环境（可在单机进行伪分布式实验）
3. Hadoop Yarn环境（可在单机进行伪分布式实验）

Spark环境和Hadoop环境是以别人的dockerfile为基础改的这是Github的地址：https://github.com/big-data-europe/docker-hadoop

# 实验记录 2020-02-27

lmh将进行过预处理的数据写入了Mongodb，数据可以正常写入，并通过Mongodb读取。

让xxk学习通过SpringBoot-jpa组件读取Mongodb。

# 实验记录 2020-03-10

将之前的研究成果进行整理

建立远程仓库：https://github.com/FHamster/SparkExp

# 实验记录 2020-03-11

## 发现了title字段空白的错误

翻看mongodb内的数据时发现了严重的错误，大量的title字段一片空白。

排查以后发现是spark-xml推导出了不期望的schema，导致title节点的文本全部没读。

spark-xml推测的schema：

```
root
 |-- _cdate: string (nullable = true)
 |-- _key: string (nullable = true)
 |-- _mdate: string (nullable = true)
 |-- _publtype: string (nullable = true)
 |-- author: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _aux: string (nullable = true)
 |    |    |-- _orcid: string (nullable = true)
 |-- booktitle: string (nullable = true)
 |-- cdrom: string (nullable = true)
 |-- cite: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _label: string (nullable = true)
 |-- crossref: string (nullable = true)
 |-- editor: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _orcid: string (nullable = true)
 |-- ee: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _type: string (nullable = true)
 |-- journal: string (nullable = true)
 |-- month: string (nullable = true)
 |-- note: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- _VALUE: string (nullable = true)
 |    |    |-- _type: string (nullable = true)
 |-- number: string (nullable = true)
 |-- pages: string (nullable = true)
 |-- publisher: string (nullable = true)
 |-- title: struct (nullable = true)
 |    |-- _VALUE: string (nullable = true)
 |    |-- _bibtex: string (nullable = true)
 |    |-- i: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sub: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sup: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |-- url: string (nullable = true)
 |-- volume: string (nullable = true)
 |-- year: long (nullable = true)

```

定位了导致这种现象的xml记录

```xml
<title>&#120001;<sub>0</sub> Regularized Structured Sparsity Convolutional Neural Networks.</title>

<title>Beyond Ohba's Conjecture: A bound on the choice number of <i>k</i>-chromatic graphs with <i>n</i> vertices.</title>

<title>Asynchronous machine vector control: PI<sup>&#945;</sup> controllers for current loops.</title>
```



在dblp网站的显示效果

* **𝓁**0 Regularized Structured Sparsity Convolutional Neural Networks.
* Beyond Ohba's Conjecture: A bound on the choice number of *k*-chromatic graphs with *n* vertices.
* Average dwell time approach to *H*∞ filter for continuous-time switched linear parameter varying systems with time-varying delay.

**发现文章的标题有特殊的文本效果，但是spark-xml在处理时丢失了这部分的语意。需要进行进一步的预处理。并且这意味**

1. 着其他的dblp下的一级字节点也会有这个问题，需要人工设定不少的schema （头疼）
2. 网页显示工作需要加入这种显示功能 （头更疼）







# 实验记录 2020-04-08

## 尝试解决title空白问题，未成功

尝试手动设定schema以后仍然产生这样的现象，**效果不好**

```json
[
  {
    "title": " Transferability of Adversarial Examples to Attack Cloud-based Image Classifier Service."
  },
  {
    "title": "! and ? - Storage as Tensorial Strength."
  },
  {
    "title": "!MDP Playground: Meta-Features in Reinforcement Learning."
  },
  {
    "title": "\""
  },
  {
    "title": "\""
  },
  {
    "title": "\""
  },
  //这里省略
  [
  {
    "title": "\uD83D\uDC4D as social support: Relational closeness, automaticity, and interpreting social support from paralinguistic digital affordances in social media."
  },
  {
    "title": "\uD835\uDD43"
  },
  {
    "title": "\uD835\uDD3D"
  },
  {
    "title": "\uD835\uDD3D"
  },
  {
    "title": "\uD835\uDCDE(k)-robust spanners in one dimension."
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCC1"
  },
  {
    "title": "\uD835\uDCB1\uD835\uDCB0-smoothness and proximal point results for some nonconvex functions."
  },
  {
    "title": "\uD835\uDCAB-schemes and Deterministic Polynomial Factoring over Finite Fields."
  },
  {
    "title": "픽"
  },
  {
    "title": "풟-stability performance analysis and stabilization of Sun-Earth "
  },
  {
    "title": "풞-Consistency in signed total graphs of commutative rings."
  }
]
]
```

目前计划通过正则表达式替换```<sub>``` ```<sup>``` ```<i>```这样的标签，在成功读取为dataframe以后再通过dataframe中进行字符串替换

## 对title字段的一些发现

```
 |-- title: struct (nullable = true)
 |    |-- _VALUE: string (nullable = true)
 |    |-- _bibtex: string (nullable = true)
 |    |-- i: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sub: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |    |-- sup: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
```

```_VALUE```:title的文本

```i```:斜体
```sup```:上标
```sub```:下标

```_bibtex```:使用bibtex语法描述的title

```tt```:补充的

# 实验记录 2020-04-16

通过替换标签字符串的方式解决了title空白问题。

开始尝试将所有dblp下的子节点写入mongodb

写入成功，目前在检查有没有信息丢失情况。



# 进展简报

已经完成了dblp全量写入mongodb数据库的工作，现在正在检查有没有数据出错的情况。

林谋瀚和高昕正在进行数据检查，还有思考优化查询优化的策略。我们目前的想法有引入杜威编码的方向、以空间换时间的方向、还有借助mongodb的索引的方向。

谢旭坤和张欣现在在进行SpringBoot查询mongodb的探索。

由于目前实验条件的限制，没有服务器集群可以使用。所以目前先用的是docker在个人电脑搭建伪分布式实验环境。现在有三套docker环境：

1. Mongodb环境
2. Spark和HDFS的环境
3. Hadoop Yarn环境

代码托管至github: https://github.com/FHamster/SparkExp.git



# 实验记录 2020-05-10

发现spark-xml进行数据模式推导的结果不是很统一。为了方便服务器取数据，我们手工设定Dataframe的schema。如下是article手工设定的Dataframe数据模式。除此之外也设置了其他的，比如inproceedings、proceedings等。

```scala
  //设定article schema的scala代码
  val articleSchema:StructType = new StructType(Array(
    StructField("_cdate", StringType, nullable = true),
    StructField("_key", StringType, nullable = true),
    StructField("_mdate", StringType, nullable = true),
    StructField("_publtype", StringType, nullable = true),
    StructField("author", ArrayType(
      StructType(Array(
        StructField("_VALUE", StringType, nullable = true),
        StructField("_orcid", StringType, nullable = true),
        StructField("_aux", StringType, nullable = true)
      )), containsNull = true)
    ),
    StructField("booktitle", StringType, nullable = true),
    StructField("cdrom", StringType, nullable = true),
    StructField("cite", ArrayType(
      StructType(Array(
        StructField("_VALUE", StringType, nullable = true),
        StructField("_label", StringType, nullable = true)
      )), containsNull = true)
    ),
    StructField("crossref", StringType, nullable = true),
    StructField("editor", ArrayType(
      StructType(Array(
        StructField("_VALUE", StringType, nullable = true),
        StructField("_orcid", StringType, nullable = true)
      )), containsNull = true)
    ),
    StructField("ee", ArrayType(
      StructType(Array(
        StructField("_VALUE", StringType, nullable = true),
        StructField("_type", StringType, nullable = true)
      )), containsNull = true)
    ),
    StructField("journal", StringType, nullable = true),
    StructField("month", StringType, nullable = true),
    StructField("note", ArrayType(
      StructType(Array(
        StructField("_VALUE", StringType, nullable = true),
        StructField("_type", StringType, nullable = true)
      )), containsNull = true)
    ),
    StructField("number", StringType, nullable = true),
    StructField("pages", StringType, nullable = true),
    StructField("publisher", StringType, nullable = true),
    StructField("title", StructType(Array(
      StructField("_VALUE", StringType, nullable = true),
      StructField("_bibtex", StringType, nullable = true),
    )), nullable = true),
    StructField("url", StringType, nullable = true),
    StructField("volume", StringType, nullable = true),
    StructField("year", LongType, nullable = true)
  ))
```

# 实验记录 2020-05-12

为了能够得到一份DBLP的所有author的列表我们在mongodb新建了一个author文档。并将作者信息导入，并进行去重。

```scala
//这里将作者信息导入author文档
test("article") {
    import com.databricks.spark.xml._
    val subnode = "article"
    val ss: SparkSession = SparkSession
      .builder
      .appName("Write_article")
      .master("local[*]")
      .config("spark.mongodb.output.uri", s"mongodb://127.0.0.1/SparkDBLPTest.Property.Author")
      .getOrCreate()

    val opt = ss.read
      .option("rootTag", "dblp")
      .option("rowTag", subnode)
      .schema(PropertiesObj.articleSchema)
      .xml(PropertiesObj.wholeDBLP_cvtSparkPath)

    import ss.implicits._
    val res = opt
      .select(explode($"author") as "author")
      .select($"author._VALUE" as "_VALUE",
        $"author._orcid" as "_orcid",
        $"author._aux" as "_aux"
      ).distinct()
      .sort($"_VALUE")

    println(s"write $subnode into mongodb")
    MongoSpark.save(res)
    ss.stop()
  }

//这里对author文档进行去重操作
test("distinct author") {
    import com.mongodb.spark.config._
    val sparkSession: SparkSession = SparkSession
      .builder
      .appName("in")
      .master("local[*]")
      .config("spark.mongodb.output.uri", s"mongodb://127.0.0.1/SparkDBLPTest.Property.Author")
      .config("spark.mongodb.input.uri", s"mongodb://127.0.0.1/SparkDBLPTest.Property.Author")
      .getOrCreate()

    import com.mongodb.spark.config._
    import sparkSession.implicits._
    val customReadConfig = ReadConfig(Map(
      "readPreference.name" -> "secondaryPreferred"),
      Some(ReadConfig(sparkSession)))
    val df = sparkSession.read.format("mongo").options(customReadConfig.asOptions).load()

    println(df.count())
    val df2 = df.dropDuplicates("_VALUE")
      .select($"_VALUE", $"_orcid")
      .cache()
    println(df2.count())
    df2.show(100)
    import com.mongodb.spark.config._

    MongoSpark.save(df2.write.option("collection", "Property.Author").mode("overwrite"))
  }
```



#  实验记录 2020-05-25

##一些mongodb查询语句的例子

根据字符串字段进行正则匹配

```json
{'title': cloud}
```

根据字符串字段进行正则匹配

```json
{'title': /cloud/i}
```

根据数组内的元素进行精确匹配

```json
{'author._VALUE': "Joon Sun Park"}
```

根据数组内的元素进行正则匹配

```json
{'author._VALUE': /Park/i}
```

根据数组内元素进行一组正则条件的匹配

```json
{
	'author._VALUE': {
		$in: [/Park/i, /Mike/i]
	}
}
```

# 实验记录 2020-06-28

##UI可以用了

我们已经将mongodb-SpringBoot-Vue连接起来了，我们现在只做了article文档的部分查询操作。

通过SpringDataMongoDB执行查询操作。

article实体类的定义

```java
package cn.jmu.spark_dblp.server.entity;

import cn.jmu.spark_dblp.server.entity.sub.*;
import lombok.Data;
import org.springframework.data.annotation.Id;
import org.springframework.data.mongodb.core.mapping.Document;
import org.springframework.data.mongodb.core.mapping.Field;

import java.util.List;

@Document(collection = "article")
@Data
public class Article {
    @Id
    protected String _id;
    @Field
    private String _cdate;
    @Field
    private String _key;
    @Field
    private String _mdate;
    @Field
    private String _publtype;
    @Field
    private List<property.Author> author;
    @Field
    private String booktitle;
    @Field
    private String cdrom;
    @Field
    private List<Cite> cite;
    @Field
    private String crossref;
    @Field
    private List<Editor> editor;
    @Field
    private List<Ee> ee;
    @Field
    private String journal;
    @Field
    private String month;
    @Field
    private List<Note> note;
    @Field
    private String publisher;
    @Field
    private String title;
    @Field
    private String url;
    @Field
    private String volume;
    @Field
    private Long year;
}
```

article的查询方法。SpringData系列的ORM框架API风格统一，并且API设计上有Hibernate的影子。这给予在写简单查询语句时很大的方便。

```java
package cn.jmu.spark_dblp.server.dao;

import cn.jmu.spark_dblp.server.entity.Article;
import cn.jmu.spark_dblp.server.entity.sub.Property.Author;
import org.springframework.data.mongodb.repository.Query;
import org.springframework.data.repository.CrudRepository;
import org.springframework.stereotype.Repository;

import java.util.List;
import java.util.regex.Pattern;
import java.util.stream.Stream;

@Repository
public interface ArticleDAO extends CrudRepository<Article, String> {
  	//根据作者进行匹配，这种查询是精确匹配，并且传入参数的设置比较麻烦。不太实用不予采用
    List<Article> findAllByAuthorContaining(List<property.Author> author);

  	//与上一个查询方法类似
    Stream<Article> findAllByAuthorContaining(property.Author author);

  	//这个是手工设定的MongoDB查询语句，但是他还结合了一些SpEL在里面。
  	//目前对article三条件查询是基于这个实现的
  	//由于三个条件可能缺失，这个语句很丑，我正在寻求写得好看的方法
  	//由于可能需要对返回的数据进行简单的规约后再给前端，所以返回的是一个Stream
    @Query("{" +
            "title: ?#{ [0].isEmpty() ?  {$exists :true} : {$regex: [0], $options: '$i'} }," +
            "'author._VALUE': ?#{ [1].size()==0 ?  {$exists :true} : {$in:[1]} }," +
            "'year': ?#{ [2].size()==0 ? {$exists :true} : {$in: [2]}  }"+
            "}")
    Stream<Article> findAllByTitleContainingAndAuthor__VALUEContainingAndYearIn(
            String title,
            List<Pattern> author,
            List<Integer> year
    );

  	//根据作者的精确查找
    @Query("{'author._VALUE': ?0}")
    Stream<Article> findAllByAuthorContainingAccurate(String author);

  	//根据标题的模糊查找，没怎么用到
    @Query("{title: ?#{ [0].isEmpty() ?  abc : {$regex: [0], $options: '$i'} }}")
    Stream<Article> findAllByTitleContaining(String title);

}

```

Controller部分的没什么好说的，省略。仅说明部分的http API

对article进行多条件筛选，查询条件通过json传输

```
POST http://host:port/article/search
```

对作者进行精确搜索

```
GET http://host:port/article/accurateAuthor?author={}
```

对作者进行模糊搜索

```
POST http://host:port/authors?_VALUE={}
```

## UI演示图片

![截屏2020-07-03 上午11.42.56](/Users/gaoxin/Library/Application Support/typora-user-images/截屏2020-07-03 上午11.44.13.png)

![截屏2020-07-03 上午11.46.02](/Users/gaoxin/Library/Application Support/typora-user-images/截屏2020-07-03 上午11.46.02.png)

# 实验记录 2020-06-30

发现提交返回较多结果的查询（大约500条返回结果）时延迟时间已经长到无法容忍，排查后发现主要的时间消耗在对大量数据的传输上。我们目前打算进行分页处理，限制返回的结果。

# 实验记录 2020-07-13

我们需要对数据进行分类，分类关键字是每个子节点的名称和_publitype属性。在此记录导入各个子节点的mongodb集合的group运算结果。

##article

```json
[
  {
    "prefix1": "journals/",
    "_publtype": "informal"
  },
  {
    "prefix1": "journals/",
    "_publtype": "edited"
  },
  {
    "prefix1": "journals/",
    "_publtype": "data"
  },
  {
    "prefix1": "journals/",
    "_publtype": "software"
  },
  {
    "prefix1": "journals/"
  },
  {
    "prefix1": "journals/",
    "_publtype": "survey"
  },
  {
    "prefix1": "journals/",
    "_publtype": "informal withdrawn"
  },
  {
    "prefix1": "conf/"
  },
  {
    "prefix1": "tr/",
    "_publtype": "informal"
  },
  {
    "prefix1": "dblpnote/",
    "_publtype": "informal"
  },
  {
    "prefix1": "journals/",
    "_publtype": "withdrawn"
  },
  {
    "prefix1": "persons/",
    "_publtype": "informal"
  },
  {
    "prefix1": "persons/"
  }
]
```

## book

```json
[
  {
    "prefix1": "phd/"
  },
  {
    "prefix1": "tr/"
  },
  {
    "prefix1": "persons/"
  },
  {
    "prefix1": "conf/"
  },
  {
    "prefix1": "books/",
    "_publtype": "habil"
  },
  {
    "prefix1": "phd/",
    "_publtype": "habil"
  },
  {
    "prefix1": "series/",
    "_publtype": "informal"
  },
  {
    "prefix1": "series/"
  },
  {
    "prefix1": "series/",
    "_publtype": "withdrawn"
  },
  {
    "prefix1": "books/"
  },
  {
    "prefix1": "reference/"
  }
]
```

## incollection

```json
[
  {
    "prefix1": "series/"
  },
  {
    "prefix1": "conf/"
  },
  {
    "prefix1": "books/"
  },
  {
    "prefix1": "journals/"
  },
  {
    "prefix1": "reference/"
  },
  {
    "prefix1": "books/",
    "_publtype": "encyclopedia"
  },
  {
    "prefix1": "reference/",
    "_publtype": "encyclopedia"
  },
  {
    "prefix1": "series/",
    "_publtype": "withdrawn"
  },
  {
    "prefix1": "series/",
    "_publtype": "encyclopedia"
  }
]
```

## inproceedings

```json
[
  {
    "prefix1": "series/"
  },
  {
    "prefix1": "conf/"
  },
  {
    "prefix1": "journals/"
  },
  {
    "prefix1": "www/"
  },
  {
    "prefix1": "persons/"
  },
  {
    "prefix1": "conf/",
    "_publtype": "withdrawn"
  },
  {
    "prefix1": "conf/",
    "_publtype": "informal"
  },
  {
    "prefix1": "journals/",
    "_publtype": "informal"
  },
  {
    "prefix1": "journals/",
    "_publtype": "withdrawn"
  }
]
```



## masterthesis

```json
[
  {
    "prefix1": "ms/"
  },
  {
    "prefix1": "phd/"
  }
]
```

## phdthesis

```json
[
  {
    "prefix1": "phd/"
  },
  {
    "prefix1": "series/"
  },
  {
    "prefix1": "books/"
  },
  {
    "prefix1": "phd/",
    "_publtype": "withdrawn"
  }
]
```

## proceedings

```json
[
  {
    "prefix1": "conf/",
    "_publtype": "withdrawn"
  },
  {
    "prefix1": "tr/",
    "_publtype": "informal"
  },
  {
    "prefix1": "conf/"
  },
  {
    "prefix1": "journals/"
  },
  {
    "prefix1": "reference/"
  },
  {
    "prefix1": "books/"
  },
  {
    "prefix1": "series/"
  }
]
```

## all InOne

```json
[
  {
    "prefix1": "reference/",
    "_publtype": "encyclopedia",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "series/",
    "_publtype": "withdrawn",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "books/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "reference/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "tr/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "edited",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "conf/",
    "_publtype": "withdrawn",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "phd/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "books/",
    "_publtype": "encyclopedia",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "conf/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "dblpnote/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "tr/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "series/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "data",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "series/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "series/",
    "_publtype": "encyclopedia",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "ms/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "persons/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "www/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "phd/",
    "_publtype": "withdrawn",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "conf/",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "survey",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "withdrawn",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "phd/",
    "_publtype": "habil",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "books/",
    "_publtype": "habil",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "persons/",
    "_publtype": "informal",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "software",
    "type_xml": "type_xml"
  },
  {
    "prefix1": "journals/",
    "_publtype": "informal withdrawn",
    "type_xml": "type_xml"
  }
]
```

